{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSEssLKgHuM2"
      },
      "source": [
        "# Implementation of Deep Deterministic Policy Gradient (DDPG)\n",
        "\n",
        "Marina Berm√∫dez Granados <br />\n",
        "<b>Access to all results:</b> https://shorturl.at/JORru"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG_j4mGhIBw0"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nytbaUYI85Ua",
        "outputId": "0dc4c034-d01a-476c-8911-d2c969f2869c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"apt-get\" no se reconoce como un comando interno o externo,\n",
            "programa o archivo por lotes ejecutable.\n",
            "\"apt-get\" no se reconoce como un comando interno o externo,\n",
            "programa o archivo por lotes ejecutable.\n"
          ]
        }
      ],
      "source": [
        "# Download gymnasium\n",
        "!pip -q install gymnasium\n",
        "!pip -q install Box2D\n",
        "!pip -q install mujoco\n",
        "\n",
        "!apt-get -q update\n",
        "!apt-get -q install -y libgl1-mesa-glx libosmesa6-dev libglfw3 patchelf\n",
        "!pip -q install imageio imageio-ffmpeg\n",
        "!pip -q install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bAVRv-KVUwRa"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "import imageio\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpy6Q2-e2uNL"
      },
      "source": [
        "<br />\n",
        "\n",
        "## Mount Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STsoDvUb2upQ",
        "outputId": "9d3d7cb9-78bc-4d13-d4f0-be5f699ee72c"
      },
      "outputs": [],
      "source": [
        "# Specify mode\n",
        "mode = \"local\"\n",
        "\n",
        "# Adjustments\n",
        "if mode == \"local\":\n",
        "    root =  \"\"\n",
        "\n",
        "elif mode == \"drive\":\n",
        "    from google.colab import drive, userdata\n",
        "    drive.mount('/content/drive')\n",
        "    root = '/content/drive/My Drive/Colab Notebooks/ATCI/'\n",
        "\n",
        "    # Build root folder\n",
        "    if not os.path.exists(root):\n",
        "        os.makedirs(root)\n",
        "\n",
        "elif mode == \"kaggle\":\n",
        "    from pathlib import Path\n",
        "    from zipfile import ZIP_DEFLATED, ZipFile\n",
        "    from os import PathLike\n",
        "    from IPython.display import FileLink\n",
        "\n",
        "    def zip_dir(zip_name, source_dir):\n",
        "        src_path = Path(source_dir).expanduser().resolve(strict=True)\n",
        "        with ZipFile(zip_name, 'w', ZIP_DEFLATED) as zf:\n",
        "            for file in src_path.rglob('*'):\n",
        "                zf.write(file, file.relative_to(src_path.parent))\n",
        "    root = \"/kaggle/working/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ68WRM-2cjG"
      },
      "source": [
        "<br />\n",
        "\n",
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDsvMxIow08Z"
      },
      "outputs": [],
      "source": [
        "def show_video(video_folder, wait_time=5):\n",
        "  mp4list = []\n",
        "  timeout = time.time() + wait_time\n",
        "\n",
        "  while time.time() < timeout and not mp4list:\n",
        "    mp4list = glob.glob(f\"{video_folder}*.mp4\")\n",
        "    time.sleep(0.5)  # check every 0.5 seconds\n",
        "\n",
        "  mp4list = glob.glob(f\"{video_folder}*.mp4\")\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    display.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def str2list(init_string):\n",
        "  print(init_string)\n",
        "  final_list = init_string.replace(\"np.float64(\", \"\")\n",
        "  final_list = final_list.replace(\"np.float32(\", \"\")\n",
        "  final_list = final_list.replace(\"device='cuda:0'\", \"\")\n",
        "  final_list = final_list.replace(\"tensor(\", \"\")\n",
        "  final_list = final_list.replace(\")\", \"\")\n",
        "  final_list = final_list.replace(\",\", \"\")\n",
        "  final_list = final_list.replace(\"[\", \"\")\n",
        "  final_list = final_list.replace(\"]\", \"\")\n",
        "\n",
        "  final_list = final_list.split(\" \")\n",
        "  final_list = [x for x in final_list if x.strip()]\n",
        "  return [float(i) for i in final_list]\n",
        "\n",
        "\n",
        "def plot_episodes(dataset, colors, root, name_experiment, show=True):\n",
        "\n",
        "  # Common parameters\n",
        "  x = range(1, len(dataset)+1)\n",
        "  lowess = sm.nonparametric.lowess\n",
        "  f = 0.05\n",
        "\n",
        "  # Rewards + Losses across Episodes\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "  ax1.plot(x, dataset[\"Accumulated Reward\"], color=colors[\"default\"][0])\n",
        "  ax1.plot(x, lowess(dataset[\"Accumulated Reward\"], x, frac=f)[:, 1], color=colors[\"mean\"][0])\n",
        "  ax1.set_title(\"Accumulated Rewards across Episodes\")\n",
        "  ax1.grid(True)\n",
        "\n",
        "  ax2.plot(x, dataset[\"Mean Actor losses\"], color=colors[\"default\"][1])\n",
        "  ax2.plot(x, lowess(dataset[\"Mean Actor losses\"], x, frac=f)[:, 1], color=colors[\"mean\"][1])\n",
        "  ax2.set_title(\"Mean Actor Loss across Episodes\")\n",
        "  ax2.grid(True)\n",
        "\n",
        "  ax3.plot(x, dataset[\"Mean Critic losses\"], color=colors[\"default\"][2])\n",
        "  ax3.plot(x, lowess(dataset[\"Mean Critic losses\"], x, frac=f)[:, 1], color=colors[\"mean\"][2])\n",
        "  ax3.set_title(\"Mean Critic Loss across Episodes\")\n",
        "  ax3.grid(True)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(f\"{root}{name_experiment}_episodic_metrics.jpg\")\n",
        "\n",
        "  if show:\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_run(row, colors, root, name_experiment, show=True, loaded=False):\n",
        "\n",
        "  # Common parameters\n",
        "  lowess = sm.nonparametric.lowess\n",
        "  f = 0.05\n",
        "\n",
        "  # Conversions\n",
        "  if loaded:\n",
        "    rewards = str2list(row[\"Rewards\"])\n",
        "    actor = str2list(row[\"Actor losses\"])\n",
        "    critic = str2list(row[\"Critic losses\"])\n",
        "  else:\n",
        "    rewards = row[\"Rewards\"]\n",
        "    actor = row[\"Actor losses\"]\n",
        "    critic = row[\"Critic losses\"]\n",
        "  x = range(1, len(rewards)+1)\n",
        "\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "  ax1.plot(x, rewards, color=colors[\"default\"][0])\n",
        "  ax1.plot(x, lowess(rewards, x, frac=f)[:, 1], color=colors[\"mean\"][0])\n",
        "  ax1.set_title(\"Rewards across Time Steps\")\n",
        "  ax1.grid(True)\n",
        "\n",
        "  ax2.plot(x, actor, color=colors[\"default\"][1])\n",
        "  ax2.plot(x, lowess(actor, x, frac=f)[:, 1], color=colors[\"mean\"][1])\n",
        "  ax2.set_title(\"Actor Loss across Time Steps\")\n",
        "  ax2.grid(True)\n",
        "\n",
        "  ax3.plot(x, critic, color=colors[\"default\"][2])\n",
        "  ax3.plot(x, lowess(critic, x, frac=f)[:, 1], color=colors[\"mean\"][2])\n",
        "  ax3.set_title(\"Critic Loss across Time Steps\")\n",
        "  ax3.grid(True)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(f\"{root}{name_experiment}_metrics.jpg\")\n",
        "\n",
        "  if show:\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_from_dataset(dataset, root, name_experiment,\n",
        "                      load_dataset=False, colors=None, show=True,\n",
        "                      max_min=True, verbose=True):\n",
        "\n",
        "  if load_dataset:\n",
        "    dataset = pd.read_csv(f\"{root}/results.csv\")\n",
        "\n",
        "  if colors is None:\n",
        "    default_colors = [\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#fdcce5\", \"#8bd3c7\"]\n",
        "    mean_colors = [\"#aa372f\", \"#32688a\", \"#558500\", \"#7b407d\", \"#a06300\", \"#9c7087\", \"#458c82\"]\n",
        "\n",
        "    colors = {\n",
        "        \"default\": default_colors,\n",
        "        \"mean\": mean_colors\n",
        "    }\n",
        "\n",
        "  # Rewards + Losses across Episodes\n",
        "  if verbose:\n",
        "    print(\"Episode Summary:\")\n",
        "  plot_episodes(dataset, colors, root, name_experiment, show)\n",
        "\n",
        "  # Early episode\n",
        "  if verbose:\n",
        "    print(\"\\n\\nEarly run(10):\")\n",
        "  plot_run(dataset.iloc[10], colors, root, f\"{name_experiment}_early\", show, loaded=load_dataset)\n",
        "\n",
        "  # Best episode\n",
        "  if max_min:\n",
        "    best = dataset.iloc[dataset[\"Accumulated Reward\"].idxmax()]\n",
        "    best_ind = dataset[\"Accumulated Reward\"].idxmax()\n",
        "  else:\n",
        "    best = dataset.iloc[dataset[\"Accumulated Reward\"].idxmin()]\n",
        "    best_ind = dataset[\"Accumulated Reward\"].idxmin()\n",
        "\n",
        "  if verbose:\n",
        "    print(f\"\\n\\nBest run({best_ind}):\")\n",
        "  plot_run(best, colors, root, f\"{name_experiment}_best\", show, loaded=load_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H0vWnB5IGvL"
      },
      "source": [
        "<br >\n",
        "\n",
        "## Deep Deterministic Policy Gradient\n",
        "\n",
        "Paper: https://arxiv.org/abs/1509.02971"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atye-aOp4vO9"
      },
      "source": [
        "<br >\n",
        "\n",
        "### Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rLR7U_HAA5ey"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, action_space, observation_space, batch_size, capacity=1000000):\n",
        "    \"\"\"\n",
        "    Replay Buffer\n",
        "    :action_space = Dimension of action space\n",
        "    :observation_space = Dimension of observation space\n",
        "    :batch_size = Batch size\n",
        "          - Batch size 16 for pixel problems\n",
        "          - Batch size 62 for low dimensional problems\n",
        "    :capacity = Maximum size of the buffer (10^6 default)\n",
        "    \"\"\"\n",
        "\n",
        "    # Buffer Capacity and Batch Size\n",
        "    self.capacity = capacity\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    # Save Buffers as numpy arrays\n",
        "    self.s = np.zeros([capacity, observation_space], dtype=np.float32)\n",
        "    self.a = np.zeros([capacity, action_space], dtype=np.float32)\n",
        "    self.r = np.zeros([capacity, 1], dtype=np.float32)\n",
        "\n",
        "    self.s_prime = np.zeros([capacity, observation_space], dtype=np.float32)\n",
        "    self.result = np.zeros([capacity, 1], dtype=bool)\n",
        "\n",
        "    # Buffer Size and Pointer\n",
        "    self.buffer_size = 0\n",
        "    self.buffer_ptr = 0\n",
        "\n",
        "  def push(self, s, a, r, s_prime, result):\n",
        "    # Ensure Pointer is within the given capacity\n",
        "    self.buffer_ptr = self.buffer_ptr % self.capacity\n",
        "\n",
        "    # Save observation space\n",
        "    self.s[self.buffer_ptr] = s\n",
        "    self.a[self.buffer_ptr] = a\n",
        "    self.r[self.buffer_ptr] = r\n",
        "\n",
        "    self.s_prime[self.buffer_ptr] = s_prime\n",
        "    self.result[self.buffer_ptr] = result\n",
        "\n",
        "    # Increase Size and move Pointer\n",
        "    self.buffer_size += 1\n",
        "    self.buffer_ptr += 1\n",
        "\n",
        "  def sample(self):\n",
        "    # Ensure Buffer Size is within the given capacity\n",
        "    self.buffer_size = min(self.buffer_size, self.capacity)\n",
        "\n",
        "    # Ensure there are enough samples\n",
        "    assert self.buffer_size >= self.batch_size, \"Not enough samples!\"\n",
        "\n",
        "    # Sample random batch\n",
        "    sample = np.random.choice(self.buffer_size, self.batch_size, replace=False)\n",
        "\n",
        "    # Return sample: s, a, r, s', result\n",
        "    return self.s[sample], self.a[sample], self.r[sample], self.s_prime[sample], self.result[sample]\n",
        "\n",
        "  def __len__(self):\n",
        "    # Return Legal Buffer Size\n",
        "    return min(self.buffer_size, self.capacity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4kcfIKn7wbM"
      },
      "source": [
        "<br >\n",
        "\n",
        "### Noise: Ornstein-Uhlenbeck process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GAcm16437w4i"
      },
      "outputs": [],
      "source": [
        "class Noise:\n",
        "  def __init__(self, action_space, mean=0.0, theta=0.15, sigma=0.2):\n",
        "    \"\"\"\n",
        "    Ornstein-Uhlenbeck process\n",
        "    :mean = Mean value\n",
        "    :theta = Theta hyperparameter (0.15 deafult)\n",
        "    :sigma = Sigma hyperparameter (0.2 default)\n",
        "    :seed = Random seed (0 default)\n",
        "    \"\"\"\n",
        "    # Action space and x\n",
        "    self.action_space = action_space\n",
        "    self.x = mean * np.ones(action_space)\n",
        "\n",
        "    # Parameters from formula\n",
        "    self.mean = mean\n",
        "    self.theta = theta\n",
        "    self.sigma = sigma\n",
        "\n",
        "  def sample(self):\n",
        "    # Compute noise\n",
        "    dx = self.theta * (self.mean - self.x) + self.sigma * np.array([random.random() for _ in range(len(self.x))])\n",
        "    self.x += dx\n",
        "\n",
        "    # Return sample\n",
        "    return self.x\n",
        "\n",
        "  def reset(self):\n",
        "    # Reset x to default initialization\n",
        "    self.x = self.mean * np.ones(self.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2YaKu1J4qTa"
      },
      "source": [
        "<br >\n",
        "\n",
        "### Actor - Critic Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fwd5ZL7WJjvC"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, input, output, init_weights=0.003):\n",
        "    \"\"\"\n",
        "    Actor network\n",
        "    :input = Input dimension\n",
        "    :output: Output dimension\n",
        "    :init_weights = Starting weights\n",
        "    \"\"\"\n",
        "    super(Actor, self).__init__()\n",
        "\n",
        "    # Layers\n",
        "    self.fc1 = nn.Linear(input, 128)\n",
        "    self.fc2 = nn.Linear(128, 128)\n",
        "    self.out = nn.Linear(128, output)\n",
        "\n",
        "    # Initialize weights\n",
        "    self._init_weights(init_weights)\n",
        "\n",
        "  def _init_weights(self, init_weights):\n",
        "    # Set initial weights\n",
        "    self.out.weight.data.uniform_(-init_weights, init_weights)\n",
        "    self.out.bias.data.uniform_(-init_weights, init_weights)\n",
        "\n",
        "  def forward(self, s):\n",
        "    # Build architecture\n",
        "    x = self.fc1(s)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    return torch.tanh(self.out(x))\n",
        "\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, input, init_weights=0.003):\n",
        "    \"\"\"\n",
        "    Critic network\n",
        "    :input = Input dimension\n",
        "    :output: Output dimension\n",
        "    :init_weights = Starting weights\n",
        "    \"\"\"\n",
        "    super(Critic, self).__init__()\n",
        "\n",
        "    # Layers\n",
        "    self.fc1 = nn.Linear(input, 128)\n",
        "    self.fc2 = nn.Linear(128, 128)\n",
        "    self.out = nn.Linear(128, 1)\n",
        "\n",
        "    # Initialize weights\n",
        "    self._init_weights(init_weights)\n",
        "\n",
        "  def _init_weights(self, init_weights):\n",
        "    # Set initial weights\n",
        "    self.out.weight.data.uniform_(-init_weights, init_weights)\n",
        "    self.out.bias.data.uniform_(-init_weights, init_weights)\n",
        "\n",
        "  def forward(self, s, a):\n",
        "    # Build architecture\n",
        "    x = torch.cat((s, a), dim=-1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    return self.out(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPC7tXcP4wVv"
      },
      "source": [
        "<br >\n",
        "\n",
        "### DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "267WXIwk4hmD"
      },
      "outputs": [],
      "source": [
        "class DDPG:\n",
        "  def __init__(self, env, M, T, hyperparameters, seed=0):\n",
        "    \"\"\"\n",
        "    Deep Deterministic Policy Gradient algorithm\n",
        "    :env = Gymnasium Environment\n",
        "    :M = Number of episodes\n",
        "    :T = Number of steps\n",
        "    :hyperparameters = Other hyperparameters\n",
        "    :seed = Random seed (0 default)\n",
        "    \"\"\"\n",
        "    # Random seed\n",
        "    self.seed = seed\n",
        "\n",
        "    # Hyperparameters\n",
        "    self.hyperparameters = hyperparameters\n",
        "    self.gamma = hyperparameters[\"gamma\"]\n",
        "    self.tau = hyperparameters[\"tau\"]\n",
        "    self.batch_size = hyperparameters[\"batch_size\"]\n",
        "\n",
        "    # Environment\n",
        "    self.env = env\n",
        "    self.observation_space = env.observation_space.shape[0]\n",
        "\n",
        "    self.action_space = env.action_space.shape[0]\n",
        "    self.action_space_low_bound = env.action_space.low[0]\n",
        "    self.action_space_high_bound = env.action_space.high[0]\n",
        "\n",
        "    # Device\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Networks and Optimizers\n",
        "    self.U_local = Actor(self.observation_space, self.action_space, self.seed).to(self.device)\n",
        "    self.U_target = Actor(self.observation_space, self.action_space, self.seed).to(self.device)\n",
        "    self.U_target.load_state_dict(self.U_local.state_dict())\n",
        "\n",
        "    self.U_optimizer = Adam(self.U_local.parameters(),\n",
        "                            lr=hyperparameters[\"actor_lr\"])\n",
        "\n",
        "    self.Q_local = Critic(self.observation_space + self.action_space, self.seed).to(self.device)\n",
        "    self.Q_target = Critic(self.observation_space + self.action_space, self.seed).to(self.device)\n",
        "    self.Q_target.load_state_dict(self.Q_local.state_dict())\n",
        "\n",
        "    self.Q_optimizer = Adam(self.Q_local.parameters(),\n",
        "                            lr=hyperparameters[\"critic_lr\"],\n",
        "                            weight_decay=hyperparameters[\"critic_wd\"])\n",
        "\n",
        "    # Replay Buffer\n",
        "    self.R = ReplayBuffer(self.action_space, self.observation_space, self.batch_size)\n",
        "\n",
        "    # Noise\n",
        "    self.N = Noise(self.action_space)\n",
        "\n",
        "    # Episodes and Time Steps\n",
        "    self.M = M\n",
        "    self.T = T\n",
        "\n",
        "  def _take_action(self, state, noise):\n",
        "    # Format state for network\n",
        "    state = torch.from_numpy(state).float().to(self.device)\n",
        "\n",
        "    # Generate action\n",
        "    self.U_local.eval()\n",
        "    with torch.no_grad():\n",
        "      action = self.U_local(state).cpu().numpy()\n",
        "    self.U_local.train()\n",
        "\n",
        "    # Add noise\n",
        "    if noise:\n",
        "      action += self.N.sample()\n",
        "\n",
        "    # Return action\n",
        "    return np.clip(action, self.action_space_low_bound, self.action_space_high_bound)\n",
        "\n",
        "  def _update(self):\n",
        "    # Ensure there are enough samples, else return -1\n",
        "    if len(self.R) < self.batch_size:\n",
        "        return -1, -1\n",
        "\n",
        "    # Sample random minibatch and format\n",
        "    s_batch, a_batch, r_batch, s_prime_batch, result_batch = self.R.sample()\n",
        "\n",
        "    s_batch = torch.from_numpy(s_batch).float().to(self.device)\n",
        "    a_batch = torch.from_numpy(a_batch).float().to(self.device)\n",
        "    r_batch = torch.from_numpy(r_batch).float().to(self.device)\n",
        "    s_prime_batch = torch.from_numpy(s_prime_batch).float().to(self.device)\n",
        "    result_batch = torch.from_numpy(result_batch).float().to(self.device)\n",
        "\n",
        "    # Activate gradient (for critic)\n",
        "    s_batch.requires_grad_()\n",
        "    a_batch.requires_grad_()\n",
        "\n",
        "    # Bellman\n",
        "    with torch.no_grad():\n",
        "      action_prime = self.U_target(s_prime_batch)\n",
        "      Q_prime = self.Q_target(s_prime_batch, action_prime)\n",
        "      Q = r_batch + (self.gamma * Q_prime * (1-result_batch))\n",
        "\n",
        "    # Update critic\n",
        "    Q_pred = self.Q_local(s_batch, a_batch)\n",
        "    Q_loss = F.mse_loss(Q_pred, Q)\n",
        "\n",
        "    self.Q_optimizer.zero_grad()\n",
        "    Q_loss.backward()\n",
        "    self.Q_optimizer.step()\n",
        "\n",
        "    # Update actor\n",
        "    a_pred = self.U_local(s_batch)\n",
        "    U_loss = -self.Q_local(s_batch, a_pred).mean()\n",
        "\n",
        "    self.U_optimizer.zero_grad()\n",
        "    U_loss.backward()\n",
        "    self.U_optimizer.step()\n",
        "\n",
        "    # Soft updates\n",
        "    self._soft_update(self.U_local, self.U_target)\n",
        "    self._soft_update(self.Q_local, self.Q_target)\n",
        "\n",
        "    # Return losses\n",
        "    return U_loss.data, Q_loss.data\n",
        "\n",
        "  def _soft_update(self, local_model, target_model):\n",
        "    # Apply soft update to local/target networks\n",
        "    for t_param, l_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      t_param.data.copy_(self.tau * l_param.data + (1.0 - self.tau) * t_param.data)\n",
        "\n",
        "\n",
        "  def train(self, root, save_every=100, load_networks=False, verbose=True):\n",
        "\n",
        "    # Load checkpoint\n",
        "    if load_networks:\n",
        "      self.U_local.load_state_dict(torch.load(f\"{root}/network_U.pth\"))\n",
        "      self.Q_local.load_state_dict(torch.load(f\"{root}/network_Q.pth\"))\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Episode loop\n",
        "    for m in range(0, self.M):\n",
        "\n",
        "      # Losses and Rewards\n",
        "      all_U_losses = []\n",
        "      all_Q_losses = []\n",
        "      all_rewards = []\n",
        "      acc_reward = 0\n",
        "\n",
        "      # Reset environment and Internal state\n",
        "      state, _ = self.env.reset()\n",
        "      self.N.reset()\n",
        "\n",
        "      # Show progress\n",
        "      if verbose:\n",
        "        print(f\"Starting episode {m+1}/{self.M}...\")\n",
        "\n",
        "      # Measure time\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Step loop\n",
        "      for t in range(0, self.T):\n",
        "\n",
        "        # Pick action + Noise\n",
        "        action = self._take_action(state, True)\n",
        "\n",
        "        # Apply action\n",
        "        next, reward, terminated, truncated, _ = self.env.step(action)\n",
        "\n",
        "        # Save in Replay Buffer\n",
        "        self.R.push(state, action, reward, next, terminated or truncated)\n",
        "\n",
        "        # Update state and rewards\n",
        "        state = next\n",
        "        acc_reward += reward\n",
        "\n",
        "        # Update networks\n",
        "        U_loss, Q_loss = self._update()\n",
        "\n",
        "        # Store results\n",
        "        all_U_losses.append(U_loss)\n",
        "        all_Q_losses.append(Q_loss)\n",
        "        all_rewards.append(reward)\n",
        "\n",
        "        # Break Episode when finishing\n",
        "        if terminated or truncated:\n",
        "          break\n",
        "\n",
        "      # Stop timer\n",
        "      tf = time.time() - t0\n",
        "\n",
        "    # Save Episode's metrics in json\n",
        "      mean_U_loss = np.mean([x.cpu().item() if torch.is_tensor(x) and x.is_cuda else x\n",
        "                            for x in all_U_losses])\n",
        "      mean_Q_loss = np.mean([x.cpu().item() if torch.is_tensor(x) and x.is_cuda else x\n",
        "                            for x in all_Q_losses])\n",
        "      mean_rewards = np.mean([x.cpu().item() if torch.is_tensor(x) and x.is_cuda else x\n",
        "                              for x in all_rewards])\n",
        "      results[m] = {\n",
        "          \"Actor losses\": all_U_losses,\n",
        "          \"Mean Actor losses\": mean_U_loss,\n",
        "          \"Critic losses\": all_Q_losses,\n",
        "          \"Mean Critic losses\": mean_Q_loss,\n",
        "          \"Rewards\": all_rewards,\n",
        "          \"Mean Rewards\": mean_rewards,\n",
        "          \"Accumulated Reward\": acc_reward,\n",
        "          \"Steps\": t,\n",
        "          \"Time\": tf,\n",
        "          \"Terminated\": terminated,\n",
        "          \"Truncated\": truncated\n",
        "      }\n",
        "\n",
        "      # Save Episode's metrics in pandas dataset\n",
        "      results_dataset = pd.DataFrame.from_dict(results, orient=\"index\",\n",
        "                                              columns=[\"Actor losses\", \"Mean Actor losses\",\n",
        "                                                        \"Critic losses\", \"Mean Critic losses\",\n",
        "                                                        \"Rewards\", \"Mean Rewards\", \"Accumulated Reward\",\n",
        "                                                        \"Steps\", \"Time\", \"Terminated\", \"Truncated\"])\n",
        "      # Show progress\n",
        "      if verbose:\n",
        "        print(f\"Results:  Actor Loss = {mean_U_loss}, Critic Loss = {mean_Q_loss}, Rewards = {mean_rewards}\")\n",
        "        print(f\"          Accumulated Reward = {acc_reward}, Time = {tf}, Time Steps = {t+1}\")\n",
        "        print(f\"          Terminated = {terminated}, Truncated = {truncated}\\n\")\n",
        "\n",
        "      # Checkpoint\n",
        "      if m % save_every == 0:\n",
        "        if verbose:\n",
        "          print(\"Checkpoint!\")\n",
        "        torch.save(self.U_local.state_dict(), f\"{root}/network_U.pth\")\n",
        "        torch.save(self.Q_local.state_dict(), f\"{root}/network_Q.pth\")\n",
        "        results_dataset.to_csv(f\"{root}/results.csv\")\n",
        "\n",
        "\n",
        "    # Save (final) networks and results\n",
        "    torch.save(self.U_local.state_dict(), f\"{root}/network_U.pth\")\n",
        "    torch.save(self.Q_local.state_dict(), f\"{root}/network_Q.pth\")\n",
        "    results_dataset.to_csv(f\"{root}/results.csv\")\n",
        "\n",
        "    return results, results_dataset\n",
        "\n",
        "\n",
        "  def test(self, M, T, root, load_networks=False, verbose=True, show=True):\n",
        "    # Load networks if necessary\n",
        "    if load_networks:\n",
        "      self.U_local.load_state_dict(torch.load(f\"{root}/network_U.pth\"))\n",
        "      self.Q_local.load_state_dict(torch.load(f\"{root}/network_Q.pth\"))\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Prepare Recording\n",
        "    reset = self.env\n",
        "\n",
        "    # Episode loop\n",
        "    for m in range(0, M):\n",
        "\n",
        "      # Rewards\n",
        "      frames = []\n",
        "      all_rewards = []\n",
        "      acc_reward = 0\n",
        "      \n",
        "      # Reset environment\n",
        "      self.env = gym.wrappers.RecordVideo(self.env, video_folder=root, \n",
        "                                    episode_trigger=lambda episode_id: True)\n",
        "      state, _ = self.env.reset()\n",
        "\n",
        "      # Measure time\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Step loop\n",
        "      for t in range(0, T):\n",
        "\n",
        "        # Pick action -Noise\n",
        "        action = self._take_action(state, False)\n",
        "\n",
        "        # Render video\n",
        "        frame = self.env.render()\n",
        "        frames.append(frame)\n",
        "\n",
        "        # Apply action\n",
        "        state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "\n",
        "        # Save rewards\n",
        "        all_rewards.append(reward)\n",
        "        acc_reward += reward\n",
        "\n",
        "        # Break Episode when finishing\n",
        "        if terminated or truncated:\n",
        "            # Take one dummy step to ensure video flush (important in Colab)\n",
        "            try:\n",
        "                self.env.step(self.env.action_space.sample())\n",
        "            except:\n",
        "                pass \n",
        "\n",
        "\n",
        "      # Stop timer\n",
        "      tf = time.time() - t0\n",
        "\n",
        "      # Save metrics in json\n",
        "      mean_rewards = np.mean([x.cpu().item() if torch.is_tensor(x) and x.is_cuda else x\n",
        "                          for x in all_rewards])\n",
        "      \n",
        "      results[m] = {\n",
        "          \"Rewards\": all_rewards,\n",
        "          \"Mean Rewards\": mean_rewards,\n",
        "          \"Accumulated Reward\": acc_reward,\n",
        "          \"Steps\": t,\n",
        "          \"Time\": tf,\n",
        "          \"Terminated\": terminated,\n",
        "          \"Truncated\": truncated\n",
        "      }\n",
        "\n",
        "      # Show results\n",
        "      if verbose:\n",
        "        print(f\"Rewards (mean) = {mean_rewards}, Accumulated Reward = {acc_reward}, Time = {tf}, Time Steps = {t+1}\")\n",
        "        print(f\"Terminated = {terminated}, Truncated = {truncated}\\n\")\n",
        "\n",
        "      # Show video\n",
        "      if show:\n",
        "        show_video(root)\n",
        "\n",
        "    # Wait to ensure proper saving \n",
        "    time.sleep(10)\n",
        "\n",
        "    # Close and Reset environment\n",
        "    self.env.close()\n",
        "    self.env = reset\n",
        "    self.env.reset()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXugmi5iJ2CP"
      },
      "source": [
        "<br >\n",
        "<br >\n",
        "\n",
        "## Environment 1: Pendulum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rXX20VE3dt7O"
      },
      "outputs": [],
      "source": [
        "# Build folder\n",
        "name_experiment = \"pendulum_s1\"\n",
        "folder_experiment = f\"{root}{name_experiment}/\"\n",
        "if not os.path.exists(folder_experiment):\n",
        "    os.makedirs(folder_experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SfJ3sp2LlHwZ"
      },
      "outputs": [],
      "source": [
        "# Prepare perparameters\n",
        "M = 800\n",
        "T = 500\n",
        "seed = 0\n",
        "\n",
        "hp = {\n",
        "    \"gamma\": 0.99,\n",
        "    \"tau\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "\n",
        "    \"actor_lr\": 0.001,\n",
        "\n",
        "    \"critic_lr\": 0.002,\n",
        "    \"critic_wd\": 0.01,\n",
        "\n",
        "    \"noise_theta\": 0.15,\n",
        "    \"noise_alpha\": 0.2,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1YfyuChY-bv",
        "outputId": "8965d65e-413c-4d65-c36c-87fe16f9fb66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action space:  Box(-2.0, 2.0, (1,), float32)\n",
            "Observation space:  Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
            "Max. in observation space:  [1. 1. 8.]\n",
            "Min. in observation space:  [-1. -1. -8.]\n"
          ]
        }
      ],
      "source": [
        "# Prepare environment\n",
        "env = gym.make('Pendulum-v1', render_mode=\"rgb_array\", g=9.81)\n",
        "env.reset(seed=seed)\n",
        "\n",
        "print(\"Action space: \", env.action_space)\n",
        "print(\"Observation space: \", env.observation_space)\n",
        "print(\"Max. in observation space: \", env.observation_space.high)\n",
        "print(\"Min. in observation space: \", env.observation_space.low)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "training_json, training_dataset = ddpg.train(folder_experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate plots\n",
        "plot_from_dataset(training_dataset, folder_experiment, \"pendulum\", load_dataset=False)\n",
        "#plot_from_dataset(None, folder_experiment, \"pendulum\", load_dataset=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xib-DHQV4qI"
      },
      "outputs": [],
      "source": [
        "# Download results in kaggle\n",
        "if mode == \"kaggle\":\n",
        "    zip_dir(f\"data.zip\", folder_experiment)\n",
        "    FileLink(f\"data.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "dzBap38r67ao",
        "outputId": "fbcf6c79-109f-42dc-aade-0e79f41aa6ae"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "testing_json = ddpg.test(3, T, folder_experiment, load_networks=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOzsXaVB-Zys"
      },
      "source": [
        "<br />\n",
        "\n",
        "Testing on different seed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbeX20wVB-jN"
      },
      "outputs": [],
      "source": [
        "seed = 10\n",
        "\n",
        "# Build folder\n",
        "name_experiment = \"pendulum_s2\"\n",
        "folder_experiment = f\"{root}{name_experiment}/\"\n",
        "if not os.path.exists(folder_experiment):\n",
        "    os.makedirs(folder_experiment)\n",
        "\n",
        "# Prepare environment\n",
        "env = gym.make('Pendulum-v1', render_mode=\"rgb_array\", g=9.81)\n",
        "env.reset(seed=seed)\n",
        "\n",
        "# Training\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "training_json, training_dataset = ddpg.train(folder_experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCscHY2IB-S8"
      },
      "outputs": [],
      "source": [
        "# Generate plots\n",
        "plot_from_dataset(training_dataset, folder_experiment, \"pendulum\", load_dataset=False)\n",
        "#plot_from_dataset(None, folder_experiment, \"pendulum\", load_dataset=True)\n",
        "\n",
        "# Download results in kaggle\n",
        "if mode == \"kaggle\":\n",
        "    zip_dir(f\"data.zip\", folder_experiment)\n",
        "    FileLink(f\"data.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mozngrRiDBx4"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "testing_json = ddpg.test(3, T, folder_experiment, load_networks=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG0B_SRdzRe8"
      },
      "source": [
        "<br />\n",
        "<br />\n",
        "\n",
        "## Environment 2: Bipedal Walking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T70eJ7ES2oGA"
      },
      "outputs": [],
      "source": [
        "# Build folder\n",
        "name_experiment = \"walking_s1\"\n",
        "folder_experiment = f\"{root}{name_experiment}/\"\n",
        "if not os.path.exists(folder_experiment):\n",
        "    os.makedirs(folder_experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfvNEuCA2oGB"
      },
      "outputs": [],
      "source": [
        "# Prepare perparameters\n",
        "M = 800\n",
        "T = 1000\n",
        "seed = 0\n",
        "\n",
        "hp = {\n",
        "    \"gamma\": 0.99,\n",
        "    \"tau\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "\n",
        "    \"actor_lr\": 0.001,\n",
        "\n",
        "    \"critic_lr\": 0.002,\n",
        "    \"critic_wd\": 0.01,\n",
        "\n",
        "    \"noise_theta\": 0.15,\n",
        "    \"noise_alpha\": 0.2,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_M2rhvb2oGB",
        "outputId": "3410b96b-d6fe-4493-97d1-89ea1476ca73"
      },
      "outputs": [],
      "source": [
        "# Prepare environment\n",
        "env = gym.make(\"BipedalWalker-v3\", hardcore=False, render_mode=\"rgb_array\")\n",
        "env.reset(seed=seed)\n",
        "\n",
        "print(\"Action space: \", env.action_space)\n",
        "print(\"Observation space: \", env.observation_space)\n",
        "print(\"Max. in observation space: \", env.observation_space.high)\n",
        "print(\"Min. in observation space: \", env.observation_space.low)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyHCSfvs2oGC",
        "outputId": "97252329-a526-415d-b85a-c085aaaebda8"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "training_json, training_dataset = ddpg.train(folder_experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1XE-43u2oGD"
      },
      "outputs": [],
      "source": [
        "# Generate plots\n",
        "plot_from_dataset(training_dataset, folder_experiment, \"walking\", load_dataset=False)\n",
        "#plot_from_dataset(None, folder_experiment, \"walking\", load_dataset=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKgQyMRj2oGE"
      },
      "outputs": [],
      "source": [
        "# Download results in kaggle\n",
        "if mode == \"kaggle\":\n",
        "    zip_dir(f\"data.zip\", folder_experiment)\n",
        "    FileLink(f\"data.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "CU-LI9Li2oGE",
        "outputId": "5a999643-61ef-4057-a11e-6c7c90272ba4"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "testing_json = ddpg.test(3, T, folder_experiment, load_networks=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGzbwlYTDHuT"
      },
      "source": [
        "<br />\n",
        "\n",
        "Testing on different seed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXHiyd6oDHuV"
      },
      "outputs": [],
      "source": [
        "seed = 10\n",
        "\n",
        "# Build folder\n",
        "name_experiment = \"walking_s2\"\n",
        "folder_experiment = f\"{root}{name_experiment}/\"\n",
        "if not os.path.exists(folder_experiment):\n",
        "    os.makedirs(folder_experiment)\n",
        "\n",
        "# Prepare environment\n",
        "env = gym.make(\"BipedalWalker-v3\", hardcore=False, render_mode=\"rgb_array\")\n",
        "env.reset(seed=seed)\n",
        "\n",
        "# Training\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "training_json, training_dataset = ddpg.train(folder_experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbWHCY_kDHuW"
      },
      "outputs": [],
      "source": [
        "# Generate plots\n",
        "plot_from_dataset(training_dataset, folder_experiment, \"walking\", load_dataset=False)\n",
        "#plot_from_dataset(None, folder_experiment, \"walking\", load_dataset=True)\n",
        "\n",
        "# Download results in kaggle\n",
        "if mode == \"kaggle\":\n",
        "    zip_dir(f\"data.zip\", folder_experiment)\n",
        "    FileLink(f\"data.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuNno0hoDHuX"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "testing_json = ddpg.test(3, T, folder_experiment, load_networks=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivk1h1vczvnG"
      },
      "source": [
        "<br />\n",
        "<br />\n",
        "\n",
        "## Environment 3: Robot Arm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CefB1Ner2x1U"
      },
      "outputs": [],
      "source": [
        "# Build folder\n",
        "name_experiment = \"robot_s1\"\n",
        "folder_experiment = f\"{root}{name_experiment}/\"\n",
        "if not os.path.exists(folder_experiment):\n",
        "    os.makedirs(folder_experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOponGdf2x1V"
      },
      "outputs": [],
      "source": [
        "# Prepare perparameters\n",
        "M = 800\n",
        "T = 500\n",
        "seed = 0\n",
        "\n",
        "hp = {\n",
        "    \"gamma\": 0.99,\n",
        "    \"tau\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "\n",
        "    \"actor_lr\": 0.001,\n",
        "\n",
        "    \"critic_lr\": 0.002,\n",
        "    \"critic_wd\": 0.01,\n",
        "\n",
        "    \"noise_theta\": 0.15,\n",
        "    \"noise_alpha\": 0.2,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare environment\n",
        "env = gym.make('Pusher-v5', render_mode=\"rgb_array\")\n",
        "env.reset(seed=seed)\n",
        "\n",
        "print(\"Action space: \", env.action_space)\n",
        "print(\"Observation space: \", env.observation_space)\n",
        "print(\"Max. in observation space: \", env.observation_space.high)\n",
        "print(\"Min. in observation space: \", env.observation_space.low)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "training_json, training_dataset = ddpg.train(folder_experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate plots\n",
        "plot_from_dataset(training_dataset, folder_experiment, \"robot\", load_dataset=False)\n",
        "#plot_from_dataset(None, folder_experiment, \"robot\", load_dataset=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnxWPgEG2x1Y"
      },
      "outputs": [],
      "source": [
        "# Download results in kaggle\n",
        "if mode == \"kaggle\":\n",
        "    zip_dir(f\"data.zip\", folder_experiment)\n",
        "    FileLink(f\"data.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test\n",
        "d = Display(visible=0, size=(1400, 900))\n",
        "d.start()\n",
        "\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "testing_json = ddpg.test(3, T, folder_experiment, load_networks=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QifFYHoLDoDG"
      },
      "source": [
        "<br />\n",
        "\n",
        "Testing on different seed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 10\n",
        "\n",
        "# Build folder\n",
        "name_experiment = \"robot_s2\"\n",
        "folder_experiment = f\"{root}{name_experiment}/\"\n",
        "if not os.path.exists(folder_experiment):\n",
        "    os.makedirs(folder_experiment)\n",
        "\n",
        "# Prepare environment\n",
        "env = gym.make('Pusher-v5', render_mode=\"rgb_array\")\n",
        "env.reset(seed=seed)\n",
        "\n",
        "# Training\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "training_json, training_dataset = ddpg.train(folder_experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate plots\n",
        "plot_from_dataset(training_dataset, folder_experiment, \"robot\", load_dataset=False)\n",
        "#plot_from_dataset(None, folder_experiment, \"robot\", load_dataset=True)\n",
        "\n",
        "# Download results in kaggle\n",
        "if mode == \"kaggle\":\n",
        "    zip_dir(f\"data.zip\", folder_experiment)\n",
        "    FileLink(f\"data.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test\n",
        "d = Display(visible=0, size=(1400, 900))\n",
        "d.start()\n",
        "\n",
        "ddpg = DDPG(env, M, T, hp, seed=seed)\n",
        "testing_json = ddpg.test(3, T, folder_experiment, load_networks=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyywa2Ne6yWB"
      },
      "source": [
        "<br />"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CG_j4mGhIBw0",
        "Gpy6Q2-e2uNL",
        "WJ68WRM-2cjG",
        "2H0vWnB5IGvL",
        "Atye-aOp4vO9",
        "m4kcfIKn7wbM",
        "M2YaKu1J4qTa",
        "tPC7tXcP4wVv",
        "xXugmi5iJ2CP"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
